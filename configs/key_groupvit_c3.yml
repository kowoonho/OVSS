data:
  batch_size: 16
  pin_memory: true
  num_workers: 4
  # Thomas said it should be at least about 5-10x your batch size; beyond that,
  # the differences become academic.
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    DATA: /workspace/Dataset
    meta:
      gcc3m:
        type: img_txt_pair
        path: ${data.dataset.DATA}/local_data/gcc3m_shards
        prefix: gcc-train-{000000..000331}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: ${data.dataset.DATA}/local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001242}.tar
        length: 11156203
      
      # yfcc14m:
      #   type: img_txt_pair
      #   path: /workspace/Dataset/local_data/yfcc14m_shards
      #   prefix: yfcc14m-{000000..001888}.tar
      #   length: 14615499
      # redcap12m:
      #   type: img_txt_pair
      #   path: /workspace/Dataset/local_data/redcap12m_shards
      #   prefix: redcap12m-{000000..001211}.tar
      #   length: 11866987
      imagenet:
        type: img_cls_pair
        path: ${data.dataset.DATA}/local_data/imagenet_shards
        prefix: imagenet-val-{000000..000009}.tar
        length: 50000
    train:
      - gcc3m
      # - gcc12m
      # - yfcc14m
    val:
      - imagenet

  img_aug:
    deit_aug: true
    img_size: 224
    img_scale: [0.08, 1.0]
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: 'rand-m9-mstd0.5-inc1'
    re_prob: 0.25
    re_mode: 'pixel'
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    key_label: 0
    word_type: ['noun'] # list

train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 1.6e-3
  weight_decay: 0.05
  warmup_lr: 4e-6
  min_lr: 4e-5
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0

  lr_scheduler:
    name: cosine

  optimizer:
    name: adamw
    eps: 1e-8
    betas: [0.9, 0.999]

evaluate:
  eval_only: false
  eval_freq: 1
  task:
    # - cls
    - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []

checkpoint:
  auto_resume: true
  resume: ''
  freq: 1
  max_kept: -1
  save_freq: 1


model_name: '' # display name in the logger
output: ???
tag: default
print_freq: 10
seed: 0
wandb: true
local_rank: ???
vis: []

model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads: [6, 6, 6]
    depths: [6, 3, 3]
    num_group_tokens: [64, 8, 0]
    num_output_groups: [64, 8]
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  clip_encoder: 'ViT-B/32'
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}
  key_label: ${data.text_aug.key_label}
  with_multi_label_loss: True
  with_key_token: True
  with_hard_negative_sample: False
  with_key_label: False
  K: 16
