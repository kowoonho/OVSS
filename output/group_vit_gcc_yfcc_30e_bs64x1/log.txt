[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 443): INFO Scale base_lr from 0.0016 to 2.5e-05
[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 444): INFO Scale warmup_lr from 4e-06 to 6.25e-08
[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 445): INFO Scale min_lr from 4e-05 to 6.25e-07
[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 453): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs64x1/config.json
[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 459): INFO Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.8.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0+cu111
OpenCV: 4.4.0
MMCV: 1.3.14
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
------------------------------------------------------------

[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 461): INFO Git hash: acd8aa3
[2023-11-07 13:55:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 464): INFO data:
  batch_size: 64
  pin_memory: true
  num_workers: 0
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: /workspace/Dataset/local_data/gcc3m_shards
        prefix: gcc-train-{000000..000331}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: /workspace/Dataset/local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001242}.tar
        length: 11156203
      imagenet:
        type: img_cls_pair
        path: /workspace/Dataset/local_data/imagenet_shards
        prefix: imagenet-val-{000000..000009}.tar
        length: 50000
    train:
    - gcc3m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 2.5e-05
  weight_decay: 0.05
  warmup_lr: 6.25e-08
  min_lr: 6.25e-07
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: false
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: ''
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs64x1
output: output/group_vit_gcc_yfcc_30e_bs64x1
tag: default
print_freq: 10
seed: 0
wandb: true
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-07 13:55:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 112): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs64x1
[2023-11-07 13:55:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 117): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-07 13:55:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 126): INFO number of params: 55726610
[2023-11-07 13:55:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 138): INFO no checkpoint found in output/group_vit_gcc_yfcc_30e_bs64x1, ignoring auto resume
[2023-11-07 13:55:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 155): INFO Start training
[2023-11-07 13:55:54 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][0/45178]	eta 15:23:32 lr 0.000000	time 1.2265 (1.2265)	total_loss 9.3948 (9.3948)	loss 4.4210 (4.4210)	multi_label_loss 4.9738 (4.9738)	grad_norm inf (inf)	mem 11359MB
[2023-11-07 13:55:59 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][10/45178]	eta 7:54:06 lr 0.000000	time 0.5717 (0.6298)	total_loss 9.2708 (9.3008)	loss 4.3725 (4.3597)	multi_label_loss 4.8983 (4.9411)	grad_norm 42.7750 (inf)	mem 11359MB
[2023-11-07 13:56:05 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][20/45178]	eta 7:29:11 lr 0.000000	time 0.5607 (0.5968)	total_loss 9.5427 (9.3048)	loss 4.5401 (4.3601)	multi_label_loss 5.0025 (4.9447)	grad_norm 41.8047 (inf)	mem 11359MB
[2023-11-07 13:56:11 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][30/45178]	eta 7:20:33 lr 0.000000	time 0.5633 (0.5855)	total_loss 9.3359 (9.2974)	loss 4.3974 (4.3571)	multi_label_loss 4.9385 (4.9403)	grad_norm 40.6579 (inf)	mem 11359MB
[2023-11-07 13:56:16 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][40/45178]	eta 7:15:47 lr 0.000000	time 0.5595 (0.5793)	total_loss 9.2240 (9.2930)	loss 4.2689 (4.3547)	multi_label_loss 4.9551 (4.9383)	grad_norm 40.5862 (inf)	mem 11359MB
[2023-11-07 13:56:22 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][50/45178]	eta 7:15:12 lr 0.000000	time 0.5670 (0.5786)	total_loss 9.2728 (9.2930)	loss 4.3443 (4.3547)	multi_label_loss 4.9285 (4.9383)	grad_norm 42.6792 (inf)	mem 11359MB
[2023-11-07 13:56:28 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][60/45178]	eta 7:13:24 lr 0.000000	time 0.5548 (0.5764)	total_loss 9.2378 (9.2901)	loss 4.2691 (4.3539)	multi_label_loss 4.9687 (4.9362)	grad_norm 41.6477 (inf)	mem 11359MB
[2023-11-07 13:56:33 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][70/45178]	eta 7:12:28 lr 0.000000	time 0.5451 (0.5753)	total_loss 9.3385 (9.2977)	loss 4.4121 (4.3578)	multi_label_loss 4.9264 (4.9399)	grad_norm 42.3576 (inf)	mem 11359MB
[2023-11-07 13:56:39 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][80/45178]	eta 7:11:08 lr 0.000000	time 0.5602 (0.5736)	total_loss 9.2356 (9.3007)	loss 4.3406 (4.3612)	multi_label_loss 4.8950 (4.9395)	grad_norm 41.9305 (inf)	mem 11359MB
[2023-11-07 13:56:45 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][90/45178]	eta 7:11:14 lr 0.000000	time 0.5597 (0.5739)	total_loss 9.1762 (9.2915)	loss 4.2882 (4.3573)	multi_label_loss 4.8880 (4.9341)	grad_norm 40.9236 (inf)	mem 11359MB
[2023-11-07 13:56:50 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][100/45178]	eta 7:10:16 lr 0.000000	time 0.5567 (0.5727)	total_loss 9.2996 (9.2921)	loss 4.3745 (4.3589)	multi_label_loss 4.9250 (4.9332)	grad_norm 41.5753 (inf)	mem 11359MB
[2023-11-07 13:56:56 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][110/45178]	eta 7:10:09 lr 0.000000	time 0.5728 (0.5727)	total_loss 9.2177 (9.2869)	loss 4.3118 (4.3550)	multi_label_loss 4.9059 (4.9319)	grad_norm 40.5416 (inf)	mem 11359MB
[2023-11-07 13:57:02 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][120/45178]	eta 7:09:54 lr 0.000000	time 0.5624 (0.5725)	total_loss 9.2939 (9.2874)	loss 4.3725 (4.3555)	multi_label_loss 4.9215 (4.9318)	grad_norm 40.7372 (inf)	mem 11359MB
[2023-11-07 13:57:07 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][130/45178]	eta 7:08:58 lr 0.000000	time 0.5495 (0.5713)	total_loss 9.2059 (9.2818)	loss 4.4049 (4.3542)	multi_label_loss 4.8009 (4.9275)	grad_norm 43.0301 (inf)	mem 11359MB
[2023-11-07 13:57:13 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][140/45178]	eta 7:08:09 lr 0.000000	time 0.5589 (0.5704)	total_loss 9.1078 (9.2760)	loss 4.2886 (4.3507)	multi_label_loss 4.8192 (4.9253)	grad_norm 39.9411 (inf)	mem 11359MB
[2023-11-07 13:57:18 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][150/45178]	eta 7:07:28 lr 0.000000	time 0.5560 (0.5696)	total_loss 9.4060 (9.2746)	loss 4.4259 (4.3490)	multi_label_loss 4.9801 (4.9255)	grad_norm 43.2996 (inf)	mem 11359MB
[2023-11-07 13:57:24 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][160/45178]	eta 7:08:41 lr 0.000000	time 0.6217 (0.5714)	total_loss 9.4194 (9.2739)	loss 4.5189 (4.3497)	multi_label_loss 4.9005 (4.9242)	grad_norm 40.4530 (inf)	mem 11359MB
[2023-11-07 13:57:30 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][170/45178]	eta 7:08:24 lr 0.000000	time 0.5662 (0.5711)	total_loss 9.0314 (9.2690)	loss 4.1835 (4.3467)	multi_label_loss 4.8479 (4.9222)	grad_norm 39.9776 (inf)	mem 11359MB
[2023-11-07 13:57:36 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][180/45178]	eta 7:08:02 lr 0.000000	time 0.5602 (0.5708)	total_loss 9.3245 (9.2679)	loss 4.4406 (4.3477)	multi_label_loss 4.8839 (4.9202)	grad_norm 41.4979 (inf)	mem 11359MB
[2023-11-07 13:57:41 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][190/45178]	eta 7:07:33 lr 0.000000	time 0.5468 (0.5702)	total_loss 9.2451 (9.2644)	loss 4.3789 (4.3466)	multi_label_loss 4.8662 (4.9178)	grad_norm 40.4966 (inf)	mem 11359MB
[2023-11-07 13:57:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][200/45178]	eta 7:07:28 lr 0.000000	time 0.5788 (0.5702)	total_loss 9.2092 (9.2628)	loss 4.2922 (4.3463)	multi_label_loss 4.9171 (4.9165)	grad_norm 40.8900 (inf)	mem 11359MB
[2023-11-07 13:57:53 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][210/45178]	eta 7:07:55 lr 0.000000	time 0.5580 (0.5710)	total_loss 9.3373 (9.2611)	loss 4.4422 (4.3465)	multi_label_loss 4.8952 (4.9146)	grad_norm 43.9985 (inf)	mem 11359MB
[2023-11-07 13:57:58 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][220/45178]	eta 7:07:23 lr 0.000000	time 0.5423 (0.5704)	total_loss 9.1633 (9.2581)	loss 4.2884 (4.3454)	multi_label_loss 4.8749 (4.9128)	grad_norm 41.2084 (inf)	mem 11359MB
[2023-11-07 13:58:04 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][230/45178]	eta 7:06:42 lr 0.000000	time 0.5460 (0.5696)	total_loss 9.2910 (9.2563)	loss 4.3829 (4.3452)	multi_label_loss 4.9082 (4.9111)	grad_norm 40.4545 (inf)	mem 11359MB
[2023-11-07 13:58:10 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][240/45178]	eta 7:06:21 lr 0.000000	time 0.5478 (0.5693)	total_loss 9.1624 (9.2549)	loss 4.3120 (4.3454)	multi_label_loss 4.8504 (4.9095)	grad_norm 41.9706 (inf)	mem 11359MB
[2023-11-07 13:58:15 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][250/45178]	eta 7:05:36 lr 0.000000	time 0.5534 (0.5684)	total_loss 9.2001 (9.2526)	loss 4.2676 (4.3433)	multi_label_loss 4.9326 (4.9094)	grad_norm 40.2898 (inf)	mem 11359MB
[2023-11-07 13:58:21 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][260/45178]	eta 7:05:12 lr 0.000000	time 0.5589 (0.5680)	total_loss 9.0754 (9.2519)	loss 4.2155 (4.3437)	multi_label_loss 4.8599 (4.9083)	grad_norm 38.3909 (inf)	mem 11359MB
[2023-11-07 13:58:26 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][270/45178]	eta 7:04:42 lr 0.000000	time 0.5633 (0.5674)	total_loss 9.2042 (9.2485)	loss 4.3599 (4.3423)	multi_label_loss 4.8443 (4.9062)	grad_norm 41.5434 (inf)	mem 11359MB
[2023-11-07 13:58:32 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][280/45178]	eta 7:04:22 lr 0.000000	time 0.5526 (0.5671)	total_loss 9.2467 (9.2460)	loss 4.3370 (4.3409)	multi_label_loss 4.9097 (4.9051)	grad_norm 42.1463 (inf)	mem 11359MB
[2023-11-07 13:58:37 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][290/45178]	eta 7:04:13 lr 0.000000	time 0.5655 (0.5670)	total_loss 9.1266 (9.2434)	loss 4.2634 (4.3395)	multi_label_loss 4.8632 (4.9039)	grad_norm 41.6391 (inf)	mem 11359MB
[2023-11-07 13:58:44 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][300/45178]	eta 7:05:07 lr 0.000000	time 0.5903 (0.5684)	total_loss 9.2613 (9.2413)	loss 4.3939 (4.3388)	multi_label_loss 4.8675 (4.9025)	grad_norm 40.7045 (inf)	mem 11359MB
[2023-11-07 13:58:49 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][310/45178]	eta 7:05:02 lr 0.000000	time 0.5464 (0.5684)	total_loss 9.1139 (9.2392)	loss 4.2896 (4.3381)	multi_label_loss 4.8242 (4.9011)	grad_norm 40.7935 (inf)	mem 11359MB
[2023-11-07 13:58:55 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][320/45178]	eta 7:04:40 lr 0.000000	time 0.5421 (0.5680)	total_loss 9.2262 (9.2375)	loss 4.3495 (4.3375)	multi_label_loss 4.8766 (4.9001)	grad_norm 39.6204 (inf)	mem 11359MB
[2023-11-07 13:59:00 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][330/45178]	eta 7:04:29 lr 0.000000	time 0.5533 (0.5679)	total_loss 9.3244 (9.2356)	loss 4.3921 (4.3368)	multi_label_loss 4.9323 (4.8988)	grad_norm 40.2413 (inf)	mem 11359MB
[2023-11-07 13:59:06 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][340/45178]	eta 7:04:09 lr 0.000000	time 0.5663 (0.5676)	total_loss 9.1230 (9.2337)	loss 4.2387 (4.3362)	multi_label_loss 4.8843 (4.8975)	grad_norm 40.8470 (inf)	mem 11359MB
[2023-11-07 13:59:12 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][350/45178]	eta 7:04:03 lr 0.000000	time 0.5598 (0.5676)	total_loss 9.2992 (9.2329)	loss 4.3964 (4.3363)	multi_label_loss 4.9028 (4.8966)	grad_norm 43.0014 (inf)	mem 11359MB
[2023-11-07 13:59:17 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][360/45178]	eta 7:03:46 lr 0.000000	time 0.5557 (0.5673)	total_loss 9.2726 (9.2322)	loss 4.3998 (4.3362)	multi_label_loss 4.8728 (4.8960)	grad_norm 41.0271 (inf)	mem 11359MB
[2023-11-07 13:59:23 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][370/45178]	eta 7:03:24 lr 0.000000	time 0.5613 (0.5670)	total_loss 9.1921 (9.2301)	loss 4.3144 (4.3351)	multi_label_loss 4.8778 (4.8949)	grad_norm 38.4024 (inf)	mem 11359MB
[2023-11-07 13:59:29 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][380/45178]	eta 7:03:34 lr 0.000000	time 0.5888 (0.5673)	total_loss 9.1287 (9.2276)	loss 4.2316 (4.3342)	multi_label_loss 4.8971 (4.8934)	grad_norm 39.3541 (inf)	mem 11359MB
[2023-11-07 13:59:34 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][390/45178]	eta 7:03:25 lr 0.000000	time 0.5780 (0.5672)	total_loss 9.2111 (9.2267)	loss 4.3078 (4.3338)	multi_label_loss 4.9032 (4.8929)	grad_norm 39.7651 (inf)	mem 11359MB
[2023-11-07 13:59:40 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][400/45178]	eta 7:03:13 lr 0.000000	time 0.5437 (0.5671)	total_loss 9.3258 (9.2260)	loss 4.4888 (4.3340)	multi_label_loss 4.8370 (4.8920)	grad_norm 39.1430 (inf)	mem 11359MB
[2023-11-07 13:59:45 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][410/45178]	eta 7:02:48 lr 0.000000	time 0.5461 (0.5667)	total_loss 9.1974 (9.2238)	loss 4.2853 (4.3326)	multi_label_loss 4.9121 (4.8912)	grad_norm 39.4394 (inf)	mem 11359MB
[2023-11-07 13:59:51 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][420/45178]	eta 7:02:33 lr 0.000000	time 0.5638 (0.5665)	total_loss 9.0710 (9.2230)	loss 4.2320 (4.3326)	multi_label_loss 4.8390 (4.8904)	grad_norm 37.0012 (inf)	mem 11359MB
[2023-11-07 13:59:56 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][430/45178]	eta 7:02:12 lr 0.000000	time 0.5536 (0.5661)	total_loss 9.3234 (9.2224)	loss 4.4493 (4.3327)	multi_label_loss 4.8741 (4.8897)	grad_norm 37.6402 (inf)	mem 11359MB
[2023-11-07 14:00:02 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][440/45178]	eta 7:01:44 lr 0.000000	time 0.5419 (0.5656)	total_loss 9.0680 (9.2201)	loss 4.2215 (4.3313)	multi_label_loss 4.8465 (4.8887)	grad_norm 40.0681 (inf)	mem 11359MB
[2023-11-07 14:00:08 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][450/45178]	eta 7:01:38 lr 0.000000	time 0.5576 (0.5656)	total_loss 9.1627 (9.2188)	loss 4.3582 (4.3308)	multi_label_loss 4.8045 (4.8880)	grad_norm 36.9700 (inf)	mem 11359MB
[2023-11-07 14:00:13 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][460/45178]	eta 7:01:21 lr 0.000000	time 0.5922 (0.5653)	total_loss 9.1882 (9.2174)	loss 4.3317 (4.3304)	multi_label_loss 4.8565 (4.8870)	grad_norm 42.2362 (inf)	mem 11359MB
[2023-11-07 14:00:19 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][470/45178]	eta 7:01:04 lr 0.000000	time 0.5687 (0.5651)	total_loss 9.1687 (9.2155)	loss 4.3722 (4.3296)	multi_label_loss 4.7965 (4.8858)	grad_norm 40.7178 (inf)	mem 11359MB
[2023-11-07 14:00:24 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][480/45178]	eta 7:00:47 lr 0.000000	time 0.5520 (0.5648)	total_loss 9.0937 (9.2138)	loss 4.2763 (4.3291)	multi_label_loss 4.8174 (4.8847)	grad_norm 40.8836 (inf)	mem 11359MB
[2023-11-07 14:00:30 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][490/45178]	eta 7:00:34 lr 0.000000	time 0.5556 (0.5647)	total_loss 9.0834 (9.2122)	loss 4.2518 (4.3284)	multi_label_loss 4.8315 (4.8838)	grad_norm 39.8531 (inf)	mem 11359MB
[2023-11-07 14:00:35 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][500/45178]	eta 7:00:13 lr 0.000000	time 0.5537 (0.5643)	total_loss 9.0032 (9.2095)	loss 4.2055 (4.3270)	multi_label_loss 4.7977 (4.8826)	grad_norm 38.1252 (inf)	mem 11359MB
[2023-11-07 14:00:41 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][510/45178]	eta 7:00:11 lr 0.000000	time 0.5514 (0.5644)	total_loss 9.0797 (9.2078)	loss 4.2865 (4.3264)	multi_label_loss 4.7932 (4.8814)	grad_norm 36.7564 (inf)	mem 11359MB
[2023-11-07 14:00:46 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][520/45178]	eta 6:59:52 lr 0.000000	time 0.5433 (0.5641)	total_loss 9.1018 (9.2060)	loss 4.2277 (4.3257)	multi_label_loss 4.8742 (4.8802)	grad_norm 39.1230 (inf)	mem 11359MB
[2023-11-07 14:00:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][530/45178]	eta 6:59:35 lr 0.000000	time 0.5476 (0.5639)	total_loss 9.0919 (9.2042)	loss 4.2314 (4.3247)	multi_label_loss 4.8605 (4.8795)	grad_norm 38.7675 (inf)	mem 11359MB
[2023-11-07 14:00:57 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][540/45178]	eta 6:59:16 lr 0.000000	time 0.5503 (0.5636)	total_loss 9.0703 (9.2031)	loss 4.2474 (4.3245)	multi_label_loss 4.8228 (4.8787)	grad_norm 37.5824 (inf)	mem 11359MB
[2023-11-07 14:01:03 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][550/45178]	eta 6:59:01 lr 0.000000	time 0.5561 (0.5634)	total_loss 9.1677 (9.2025)	loss 4.3133 (4.3242)	multi_label_loss 4.8544 (4.8783)	grad_norm 37.7413 (inf)	mem 11359MB
[2023-11-07 14:01:08 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][560/45178]	eta 6:58:45 lr 0.000000	time 0.5505 (0.5631)	total_loss 9.2325 (9.2017)	loss 4.3909 (4.3243)	multi_label_loss 4.8416 (4.8774)	grad_norm 38.6927 (inf)	mem 11359MB
[2023-11-07 14:01:14 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][570/45178]	eta 6:58:32 lr 0.000000	time 0.5557 (0.5630)	total_loss 9.0451 (9.2001)	loss 4.1946 (4.3237)	multi_label_loss 4.8505 (4.8763)	grad_norm 37.7259 (inf)	mem 11359MB
[2023-11-07 14:01:19 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][580/45178]	eta 6:58:24 lr 0.000000	time 0.5755 (0.5629)	total_loss 9.1613 (9.1991)	loss 4.3252 (4.3233)	multi_label_loss 4.8362 (4.8758)	grad_norm 38.0584 (inf)	mem 11359MB
[2023-11-07 14:01:25 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][590/45178]	eta 6:58:15 lr 0.000000	time 0.5436 (0.5628)	total_loss 9.1348 (9.1978)	loss 4.2789 (4.3226)	multi_label_loss 4.8560 (4.8752)	grad_norm 38.4650 (inf)	mem 11359MB
[2023-11-07 14:01:31 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][600/45178]	eta 6:58:10 lr 0.000000	time 0.5540 (0.5628)	total_loss 9.0415 (9.1956)	loss 4.2271 (4.3216)	multi_label_loss 4.8144 (4.8741)	grad_norm 37.7268 (inf)	mem 11359MB
[2023-11-07 14:01:36 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][610/45178]	eta 6:57:59 lr 0.000000	time 0.5628 (0.5627)	total_loss 9.0854 (9.1941)	loss 4.2933 (4.3208)	multi_label_loss 4.7921 (4.8733)	grad_norm 37.7445 (inf)	mem 11359MB
[2023-11-07 14:01:42 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][620/45178]	eta 6:57:49 lr 0.000000	time 0.5516 (0.5626)	total_loss 9.1175 (9.1925)	loss 4.2715 (4.3202)	multi_label_loss 4.8461 (4.8723)	grad_norm 38.1584 (inf)	mem 11359MB
[2023-11-07 14:01:47 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][630/45178]	eta 6:57:37 lr 0.000000	time 0.5479 (0.5625)	total_loss 9.0928 (9.1915)	loss 4.3009 (4.3197)	multi_label_loss 4.7918 (4.8718)	grad_norm 37.7322 (inf)	mem 11359MB
[2023-11-07 14:01:53 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][640/45178]	eta 6:57:28 lr 0.000000	time 0.5572 (0.5624)	total_loss 9.1311 (9.1901)	loss 4.3290 (4.3191)	multi_label_loss 4.8021 (4.8711)	grad_norm 40.0982 (inf)	mem 11359MB
[2023-11-07 14:01:59 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][650/45178]	eta 6:57:26 lr 0.000000	time 0.5833 (0.5625)	total_loss 9.1170 (9.1885)	loss 4.3105 (4.3181)	multi_label_loss 4.8065 (4.8704)	grad_norm 37.7498 (inf)	mem 11359MB
[2023-11-07 14:02:04 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][660/45178]	eta 6:57:13 lr 0.000000	time 0.5562 (0.5623)	total_loss 9.0264 (9.1868)	loss 4.2050 (4.3172)	multi_label_loss 4.8214 (4.8696)	grad_norm 34.8514 (inf)	mem 11359MB
[2023-11-07 14:02:10 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][670/45178]	eta 6:56:59 lr 0.000000	time 0.5414 (0.5621)	total_loss 9.0438 (9.1860)	loss 4.2270 (4.3169)	multi_label_loss 4.8167 (4.8691)	grad_norm 35.0538 (inf)	mem 11359MB
[2023-11-07 14:02:15 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][680/45178]	eta 6:56:48 lr 0.000000	time 0.5537 (0.5620)	total_loss 9.2691 (9.1850)	loss 4.3893 (4.3165)	multi_label_loss 4.8798 (4.8685)	grad_norm 37.3163 (inf)	mem 11359MB
[2023-11-07 14:02:21 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][690/45178]	eta 6:56:34 lr 0.000000	time 0.5647 (0.5618)	total_loss 9.1549 (9.1841)	loss 4.3202 (4.3161)	multi_label_loss 4.8348 (4.8680)	grad_norm 36.0259 (inf)	mem 11359MB
[2023-11-07 14:02:26 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][700/45178]	eta 6:56:31 lr 0.000000	time 0.5456 (0.5619)	total_loss 9.0442 (9.1827)	loss 4.2722 (4.3154)	multi_label_loss 4.7720 (4.8673)	grad_norm 35.9201 (inf)	mem 11359MB
[2023-11-07 14:02:32 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][710/45178]	eta 6:56:32 lr 0.000000	time 0.5534 (0.5620)	total_loss 8.9746 (9.1817)	loss 4.1953 (4.3152)	multi_label_loss 4.7793 (4.8665)	grad_norm 37.2056 (inf)	mem 11359MB
[2023-11-07 14:02:38 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][720/45178]	eta 6:56:18 lr 0.000000	time 0.5426 (0.5618)	total_loss 9.1060 (9.1805)	loss 4.2588 (4.3147)	multi_label_loss 4.8472 (4.8658)	grad_norm 36.9141 (inf)	mem 11359MB
[2023-11-07 14:02:43 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][730/45178]	eta 6:56:05 lr 0.000000	time 0.5404 (0.5617)	total_loss 9.1352 (9.1794)	loss 4.3091 (4.3142)	multi_label_loss 4.8261 (4.8652)	grad_norm 36.1667 (inf)	mem 11359MB
[2023-11-07 14:02:49 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][740/45178]	eta 6:55:54 lr 0.000000	time 0.5383 (0.5615)	total_loss 9.0409 (9.1783)	loss 4.2631 (4.3139)	multi_label_loss 4.7779 (4.8644)	grad_norm 35.5073 (inf)	mem 11359MB
[2023-11-07 14:02:54 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][750/45178]	eta 6:55:44 lr 0.000000	time 0.5528 (0.5615)	total_loss 9.0671 (9.1766)	loss 4.2207 (4.3129)	multi_label_loss 4.8463 (4.8637)	grad_norm 35.9731 (inf)	mem 11359MB
[2023-11-07 14:03:00 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][760/45178]	eta 6:55:34 lr 0.000000	time 0.5484 (0.5614)	total_loss 9.0855 (9.1754)	loss 4.2763 (4.3123)	multi_label_loss 4.8092 (4.8632)	grad_norm 36.4397 (inf)	mem 11359MB
[2023-11-07 14:03:05 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][770/45178]	eta 6:55:22 lr 0.000000	time 0.5391 (0.5612)	total_loss 9.1415 (9.1744)	loss 4.2958 (4.3118)	multi_label_loss 4.8457 (4.8626)	grad_norm 36.0769 (inf)	mem 11359MB
[2023-11-07 14:03:11 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][780/45178]	eta 6:55:11 lr 0.000000	time 0.5640 (0.5611)	total_loss 9.0844 (9.1729)	loss 4.2634 (4.3112)	multi_label_loss 4.8209 (4.8617)	grad_norm 36.7035 (inf)	mem 11359MB
[2023-11-07 14:03:16 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][790/45178]	eta 6:55:04 lr 0.000000	time 0.5442 (0.5611)	total_loss 8.9322 (9.1719)	loss 4.1473 (4.3110)	multi_label_loss 4.7848 (4.8609)	grad_norm 34.6007 (inf)	mem 11359MB
[2023-11-07 14:03:22 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][800/45178]	eta 6:54:50 lr 0.000000	time 0.5532 (0.5609)	total_loss 8.9791 (9.1699)	loss 4.2069 (4.3098)	multi_label_loss 4.7722 (4.8601)	grad_norm 38.2436 (inf)	mem 11359MB
[2023-11-07 14:03:27 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][810/45178]	eta 6:54:39 lr 0.000000	time 0.5607 (0.5608)	total_loss 9.0770 (9.1681)	loss 4.2700 (4.3089)	multi_label_loss 4.8071 (4.8592)	grad_norm 35.4848 (inf)	mem 11359MB
[2023-11-07 14:03:33 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][820/45178]	eta 6:54:29 lr 0.000000	time 0.5442 (0.5606)	total_loss 9.0913 (9.1669)	loss 4.2988 (4.3084)	multi_label_loss 4.7925 (4.8585)	grad_norm 33.6419 (inf)	mem 11359MB
[2023-11-07 14:03:38 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][830/45178]	eta 6:54:17 lr 0.000000	time 0.5365 (0.5605)	total_loss 9.0106 (9.1658)	loss 4.2064 (4.3078)	multi_label_loss 4.8042 (4.8580)	grad_norm 34.5832 (inf)	mem 11359MB
[2023-11-07 14:03:44 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][840/45178]	eta 6:54:05 lr 0.000000	time 0.5360 (0.5604)	total_loss 9.1565 (9.1649)	loss 4.3004 (4.3076)	multi_label_loss 4.8561 (4.8574)	grad_norm 33.8977 (inf)	mem 11359MB
[2023-11-07 14:03:49 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][850/45178]	eta 6:53:52 lr 0.000000	time 0.5452 (0.5602)	total_loss 8.9918 (9.1636)	loss 4.2102 (4.3066)	multi_label_loss 4.7816 (4.8569)	grad_norm 36.7391 (inf)	mem 11359MB
[2023-11-07 14:03:55 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][860/45178]	eta 6:53:40 lr 0.000000	time 0.5464 (0.5601)	total_loss 9.1007 (9.1624)	loss 4.3180 (4.3063)	multi_label_loss 4.7827 (4.8561)	grad_norm 34.0282 (inf)	mem 11359MB
[2023-11-07 14:04:00 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][870/45178]	eta 6:53:26 lr 0.000000	time 0.5504 (0.5599)	total_loss 9.0369 (9.1610)	loss 4.2622 (4.3055)	multi_label_loss 4.7747 (4.8555)	grad_norm 35.2633 (inf)	mem 11359MB
[2023-11-07 14:04:06 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][880/45178]	eta 6:53:16 lr 0.000000	time 0.5515 (0.5598)	total_loss 9.0743 (9.1595)	loss 4.2732 (4.3048)	multi_label_loss 4.8011 (4.8548)	grad_norm 35.0163 (inf)	mem 11359MB
[2023-11-07 14:04:11 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][890/45178]	eta 6:53:09 lr 0.000000	time 0.5491 (0.5597)	total_loss 9.0307 (9.1582)	loss 4.1784 (4.3041)	multi_label_loss 4.8523 (4.8541)	grad_norm 35.9717 (inf)	mem 11359MB
[2023-11-07 14:04:17 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][900/45178]	eta 6:53:02 lr 0.000000	time 0.5596 (0.5597)	total_loss 9.1241 (9.1570)	loss 4.3170 (4.3035)	multi_label_loss 4.8071 (4.8535)	grad_norm 33.9362 (inf)	mem 11359MB
[2023-11-07 14:04:22 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][910/45178]	eta 6:52:53 lr 0.000000	time 0.5567 (0.5596)	total_loss 8.9867 (9.1558)	loss 4.2498 (4.3031)	multi_label_loss 4.7370 (4.8528)	grad_norm 36.4970 (inf)	mem 11359MB
[2023-11-07 14:04:28 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][920/45178]	eta 6:52:41 lr 0.000000	time 0.5376 (0.5595)	total_loss 9.0527 (9.1548)	loss 4.2467 (4.3026)	multi_label_loss 4.8060 (4.8522)	grad_norm 34.6560 (inf)	mem 11359MB
[2023-11-07 14:04:33 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][930/45178]	eta 6:52:29 lr 0.000000	time 0.5522 (0.5593)	total_loss 8.9937 (9.1536)	loss 4.2394 (4.3019)	multi_label_loss 4.7543 (4.8517)	grad_norm 34.7488 (inf)	mem 11359MB
[2023-11-07 14:04:39 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][940/45178]	eta 6:52:21 lr 0.000000	time 0.5442 (0.5593)	total_loss 9.1452 (9.1526)	loss 4.3248 (4.3015)	multi_label_loss 4.8204 (4.8511)	grad_norm 34.8054 (inf)	mem 11359MB
[2023-11-07 14:04:44 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][950/45178]	eta 6:52:11 lr 0.000000	time 0.5510 (0.5592)	total_loss 9.0426 (9.1515)	loss 4.2348 (4.3009)	multi_label_loss 4.8078 (4.8506)	grad_norm 33.8269 (inf)	mem 11359MB
[2023-11-07 14:04:50 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][960/45178]	eta 6:52:06 lr 0.000000	time 0.5509 (0.5592)	total_loss 9.0315 (9.1505)	loss 4.2475 (4.3005)	multi_label_loss 4.7840 (4.8500)	grad_norm 34.2042 (inf)	mem 11359MB
[2023-11-07 14:04:55 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][970/45178]	eta 6:51:57 lr 0.000000	time 0.5325 (0.5591)	total_loss 8.9112 (9.1492)	loss 4.0996 (4.2998)	multi_label_loss 4.8116 (4.8495)	grad_norm 32.5669 (inf)	mem 11359MB
[2023-11-07 14:05:01 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][980/45178]	eta 6:51:46 lr 0.000000	time 0.5467 (0.5590)	total_loss 9.0069 (9.1483)	loss 4.1943 (4.2993)	multi_label_loss 4.8126 (4.8490)	grad_norm 33.8493 (inf)	mem 11359MB
[2023-11-07 14:05:06 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][990/45178]	eta 6:51:35 lr 0.000000	time 0.5353 (0.5589)	total_loss 9.0140 (9.1470)	loss 4.2146 (4.2984)	multi_label_loss 4.7994 (4.8485)	grad_norm 32.8045 (inf)	mem 11359MB
[2023-11-07 14:05:12 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1000/45178]	eta 6:51:27 lr 0.000000	time 0.5499 (0.5588)	total_loss 9.0177 (9.1462)	loss 4.2191 (4.2981)	multi_label_loss 4.7986 (4.8481)	grad_norm 35.0435 (inf)	mem 11359MB
[2023-11-07 14:05:17 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1010/45178]	eta 6:51:17 lr 0.000000	time 0.5373 (0.5587)	total_loss 9.0843 (9.1448)	loss 4.2760 (4.2973)	multi_label_loss 4.8083 (4.8475)	grad_norm 34.3170 (inf)	mem 11359MB
[2023-11-07 14:05:23 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1020/45178]	eta 6:51:06 lr 0.000000	time 0.5474 (0.5586)	total_loss 9.0854 (9.1440)	loss 4.2831 (4.2969)	multi_label_loss 4.8023 (4.8470)	grad_norm 34.4407 (inf)	mem 11359MB
[2023-11-07 14:05:28 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1030/45178]	eta 6:50:57 lr 0.000000	time 0.5598 (0.5585)	total_loss 9.0461 (9.1432)	loss 4.3017 (4.2967)	multi_label_loss 4.7444 (4.8465)	grad_norm 33.4192 (inf)	mem 11359MB
[2023-11-07 14:05:34 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1040/45178]	eta 6:50:49 lr 0.000000	time 0.5510 (0.5585)	total_loss 9.1227 (9.1422)	loss 4.3251 (4.2962)	multi_label_loss 4.7976 (4.8460)	grad_norm 32.9961 (inf)	mem 11359MB
[2023-11-07 14:05:40 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1050/45178]	eta 6:50:59 lr 0.000000	time 0.6426 (0.5588)	total_loss 9.0610 (9.1410)	loss 4.2707 (4.2956)	multi_label_loss 4.7903 (4.8454)	grad_norm 33.4609 (inf)	mem 11359MB
[2023-11-07 14:05:46 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1060/45178]	eta 6:51:03 lr 0.000000	time 0.6572 (0.5590)	total_loss 9.1048 (9.1400)	loss 4.3215 (4.2952)	multi_label_loss 4.7833 (4.8448)	grad_norm 33.8672 (inf)	mem 11359MB
[2023-11-07 14:05:51 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1070/45178]	eta 6:51:04 lr 0.000000	time 0.5515 (0.5592)	total_loss 8.9780 (9.1392)	loss 4.1916 (4.2949)	multi_label_loss 4.7865 (4.8444)	grad_norm 35.5471 (inf)	mem 11359MB
[2023-11-07 14:05:57 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1080/45178]	eta 6:51:00 lr 0.000000	time 0.5555 (0.5592)	total_loss 9.1192 (9.1383)	loss 4.3307 (4.2945)	multi_label_loss 4.7885 (4.8438)	grad_norm 33.4158 (inf)	mem 11359MB
[2023-11-07 14:06:03 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1090/45178]	eta 6:51:01 lr 0.000000	time 0.5657 (0.5594)	total_loss 8.9942 (9.1375)	loss 4.2487 (4.2942)	multi_label_loss 4.7455 (4.8434)	grad_norm 35.3397 (inf)	mem 11359MB
[2023-11-07 14:06:08 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1100/45178]	eta 6:50:54 lr 0.000000	time 0.5471 (0.5593)	total_loss 9.0907 (9.1367)	loss 4.3384 (4.2938)	multi_label_loss 4.7523 (4.8429)	grad_norm 32.8818 (inf)	mem 11359MB
[2023-11-07 14:06:14 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1110/45178]	eta 6:51:03 lr 0.000000	time 0.6573 (0.5597)	total_loss 9.0761 (9.1357)	loss 4.3190 (4.2933)	multi_label_loss 4.7571 (4.8424)	grad_norm 32.7894 (inf)	mem 11359MB
[2023-11-07 14:06:20 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1120/45178]	eta 6:51:01 lr 0.000000	time 0.6514 (0.5598)	total_loss 9.0520 (9.1350)	loss 4.2795 (4.2931)	multi_label_loss 4.7726 (4.8420)	grad_norm 33.2908 (inf)	mem 11359MB
[2023-11-07 14:06:26 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1130/45178]	eta 6:51:06 lr 0.000000	time 0.6569 (0.5600)	total_loss 9.0907 (9.1340)	loss 4.2949 (4.2926)	multi_label_loss 4.7958 (4.8414)	grad_norm 33.1041 (inf)	mem 11359MB
[2023-11-07 14:06:32 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1140/45178]	eta 6:51:08 lr 0.000000	time 0.5619 (0.5602)	total_loss 8.9911 (9.1332)	loss 4.2127 (4.2923)	multi_label_loss 4.7784 (4.8409)	grad_norm 31.6151 (inf)	mem 11359MB
[2023-11-07 14:06:37 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1150/45178]	eta 6:51:07 lr 0.000000	time 0.5549 (0.5603)	total_loss 9.0137 (9.1323)	loss 4.1973 (4.2918)	multi_label_loss 4.8164 (4.8405)	grad_norm 33.8572 (inf)	mem 11359MB
[2023-11-07 14:06:43 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1160/45178]	eta 6:51:03 lr 0.000000	time 0.5516 (0.5603)	total_loss 9.0144 (9.1315)	loss 4.2250 (4.2914)	multi_label_loss 4.7894 (4.8401)	grad_norm 32.3016 (inf)	mem 11359MB
[2023-11-07 14:06:48 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1170/45178]	eta 6:50:54 lr 0.000000	time 0.5418 (0.5602)	total_loss 8.9484 (9.1307)	loss 4.1771 (4.2910)	multi_label_loss 4.7713 (4.8398)	grad_norm 33.2468 (inf)	mem 11359MB
[2023-11-07 14:06:54 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1180/45178]	eta 6:51:01 lr 0.000000	time 0.6672 (0.5605)	total_loss 8.8791 (9.1296)	loss 4.1096 (4.2904)	multi_label_loss 4.7695 (4.8393)	grad_norm 29.9860 (inf)	mem 11359MB
[2023-11-07 14:07:00 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1190/45178]	eta 6:51:05 lr 0.000000	time 0.6410 (0.5607)	total_loss 9.0444 (9.1287)	loss 4.2633 (4.2899)	multi_label_loss 4.7811 (4.8388)	grad_norm 32.1017 (inf)	mem 11359MB
[2023-11-07 14:07:06 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1200/45178]	eta 6:51:01 lr 0.000000	time 0.5514 (0.5608)	total_loss 8.8810 (9.1276)	loss 4.1403 (4.2893)	multi_label_loss 4.7407 (4.8383)	grad_norm 31.4707 (inf)	mem 11359MB
[2023-11-07 14:07:11 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1210/45178]	eta 6:50:53 lr 0.000000	time 0.5448 (0.5607)	total_loss 9.0332 (9.1266)	loss 4.2566 (4.2888)	multi_label_loss 4.7765 (4.8378)	grad_norm 30.8190 (inf)	mem 11359MB
[2023-11-07 14:07:17 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1220/45178]	eta 6:50:53 lr 0.000000	time 0.5410 (0.5608)	total_loss 8.9831 (9.1259)	loss 4.2422 (4.2885)	multi_label_loss 4.7409 (4.8374)	grad_norm 32.8190 (inf)	mem 11359MB
[2023-11-07 14:07:23 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1230/45178]	eta 6:51:00 lr 0.000000	time 0.6620 (0.5611)	total_loss 9.0156 (9.1250)	loss 4.2107 (4.2881)	multi_label_loss 4.8049 (4.8369)	grad_norm 31.2582 (inf)	mem 11359MB
[2023-11-07 14:07:29 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1240/45178]	eta 6:51:02 lr 0.000000	time 0.5511 (0.5613)	total_loss 8.9912 (9.1238)	loss 4.2467 (4.2874)	multi_label_loss 4.7445 (4.8364)	grad_norm 31.1404 (inf)	mem 11359MB
[2023-11-07 14:07:35 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1250/45178]	eta 6:50:58 lr 0.000000	time 0.6490 (0.5613)	total_loss 8.9800 (9.1231)	loss 4.2170 (4.2871)	multi_label_loss 4.7630 (4.8361)	grad_norm 33.1512 (inf)	mem 11359MB
[2023-11-07 14:07:40 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1260/45178]	eta 6:50:54 lr 0.000000	time 0.5491 (0.5614)	total_loss 8.9628 (9.1221)	loss 4.1984 (4.2866)	multi_label_loss 4.7644 (4.8356)	grad_norm 30.6913 (inf)	mem 11359MB
[2023-11-07 14:07:46 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1270/45178]	eta 6:50:52 lr 0.000000	time 0.6626 (0.5615)	total_loss 8.9618 (9.1210)	loss 4.1428 (4.2859)	multi_label_loss 4.8190 (4.8351)	grad_norm 31.8145 (inf)	mem 11359MB
[2023-11-07 14:07:52 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1280/45178]	eta 6:50:50 lr 0.000000	time 0.5612 (0.5615)	total_loss 9.0091 (9.1202)	loss 4.2083 (4.2856)	multi_label_loss 4.8008 (4.8346)	grad_norm 30.5474 (inf)	mem 11359MB
[2023-11-07 14:07:58 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1290/45178]	eta 6:50:49 lr 0.000000	time 0.6638 (0.5616)	total_loss 8.9770 (9.1193)	loss 4.1929 (4.2851)	multi_label_loss 4.7840 (4.8342)	grad_norm 31.7094 (inf)	mem 11359MB
[2023-11-07 14:08:03 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1300/45178]	eta 6:50:47 lr 0.000000	time 0.5600 (0.5617)	total_loss 8.9584 (9.1185)	loss 4.2024 (4.2848)	multi_label_loss 4.7560 (4.8337)	grad_norm 28.8337 (inf)	mem 11359MB
[2023-11-07 14:08:09 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1310/45178]	eta 6:50:49 lr 0.000000	time 0.6570 (0.5619)	total_loss 9.0662 (9.1176)	loss 4.2912 (4.2844)	multi_label_loss 4.7750 (4.8333)	grad_norm 31.0409 (inf)	mem 11359MB
[2023-11-07 14:08:15 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1320/45178]	eta 6:50:56 lr 0.000000	time 0.5543 (0.5622)	total_loss 8.9825 (9.1167)	loss 4.2387 (4.2839)	multi_label_loss 4.7438 (4.8328)	grad_norm 30.2274 (inf)	mem 11359MB
[2023-11-07 14:08:21 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1330/45178]	eta 6:50:52 lr 0.000000	time 0.5568 (0.5622)	total_loss 8.9637 (9.1157)	loss 4.2154 (4.2834)	multi_label_loss 4.7483 (4.8324)	grad_norm 31.7618 (inf)	mem 11359MB
[2023-11-07 14:08:27 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1340/45178]	eta 6:51:00 lr 0.000000	time 0.5476 (0.5625)	total_loss 9.0068 (9.1149)	loss 4.1994 (4.2830)	multi_label_loss 4.8074 (4.8320)	grad_norm 30.3939 (inf)	mem 11359MB
[2023-11-07 14:08:32 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1350/45178]	eta 6:50:54 lr 0.000000	time 0.5446 (0.5625)	total_loss 8.9397 (9.1138)	loss 4.2043 (4.2823)	multi_label_loss 4.7354 (4.8315)	grad_norm 31.4063 (inf)	mem 11359MB
[2023-11-07 14:08:38 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1360/45178]	eta 6:50:55 lr 0.000000	time 0.6649 (0.5627)	total_loss 8.9108 (9.1131)	loss 4.1533 (4.2819)	multi_label_loss 4.7575 (4.8311)	grad_norm 31.4821 (inf)	mem 11359MB
[2023-11-07 14:08:44 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1370/45178]	eta 6:50:51 lr 0.000000	time 0.5611 (0.5627)	total_loss 8.9579 (9.1121)	loss 4.2083 (4.2814)	multi_label_loss 4.7495 (4.8307)	grad_norm 30.2508 (inf)	mem 11359MB
[2023-11-07 14:08:50 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1380/45178]	eta 6:50:52 lr 0.000000	time 0.5558 (0.5629)	total_loss 9.0119 (9.1112)	loss 4.2294 (4.2809)	multi_label_loss 4.7825 (4.8303)	grad_norm 31.7242 (inf)	mem 11359MB
[2023-11-07 14:08:55 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1390/45178]	eta 6:50:50 lr 0.000000	time 0.5561 (0.5629)	total_loss 9.1098 (9.1103)	loss 4.3175 (4.2804)	multi_label_loss 4.7923 (4.8299)	grad_norm 28.9953 (inf)	mem 11359MB
[2023-11-07 14:09:01 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1400/45178]	eta 6:50:50 lr 0.000000	time 0.5591 (0.5631)	total_loss 8.9230 (9.1095)	loss 4.1736 (4.2800)	multi_label_loss 4.7494 (4.8295)	grad_norm 30.1574 (inf)	mem 11359MB
[2023-11-07 14:09:07 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1410/45178]	eta 6:50:57 lr 0.000000	time 0.6458 (0.5634)	total_loss 9.0007 (9.1087)	loss 4.2122 (4.2796)	multi_label_loss 4.7885 (4.8291)	grad_norm 29.7940 (inf)	mem 11359MB
[2023-11-07 14:09:13 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1420/45178]	eta 6:50:58 lr 0.000000	time 0.6596 (0.5635)	total_loss 9.0477 (9.1079)	loss 4.2966 (4.2792)	multi_label_loss 4.7511 (4.8286)	grad_norm 30.8162 (inf)	mem 11359MB
[2023-11-07 14:09:19 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1430/45178]	eta 6:50:48 lr 0.000000	time 0.5452 (0.5634)	total_loss 9.0037 (9.1072)	loss 4.2149 (4.2788)	multi_label_loss 4.7888 (4.8283)	grad_norm 28.8764 (inf)	mem 11359MB
[2023-11-07 14:09:25 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1440/45178]	eta 6:50:50 lr 0.000000	time 0.5677 (0.5636)	total_loss 8.9963 (9.1063)	loss 4.2278 (4.2783)	multi_label_loss 4.7685 (4.8280)	grad_norm 31.2361 (inf)	mem 11359MB
[2023-11-07 14:09:30 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1450/45178]	eta 6:50:52 lr 0.000000	time 0.6565 (0.5638)	total_loss 8.9389 (9.1055)	loss 4.1789 (4.2779)	multi_label_loss 4.7600 (4.8275)	grad_norm 29.2473 (inf)	mem 11359MB
[2023-11-07 14:09:37 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1460/45178]	eta 6:51:07 lr 0.000000	time 0.7322 (0.5642)	total_loss 9.0075 (9.1047)	loss 4.2621 (4.2775)	multi_label_loss 4.7454 (4.8272)	grad_norm 30.1983 (inf)	mem 11359MB
[2023-11-07 14:09:43 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1470/45178]	eta 6:51:15 lr 0.000000	time 0.6541 (0.5645)	total_loss 8.9538 (9.1040)	loss 4.1655 (4.2771)	multi_label_loss 4.7883 (4.8268)	grad_norm 30.2805 (inf)	mem 11359MB
[2023-11-07 14:09:49 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1480/45178]	eta 6:51:17 lr 0.000000	time 0.5687 (0.5647)	total_loss 8.9101 (9.1031)	loss 4.1794 (4.2766)	multi_label_loss 4.7307 (4.8264)	grad_norm 29.9254 (inf)	mem 11359MB
[2023-11-07 14:09:55 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1490/45178]	eta 6:51:23 lr 0.000000	time 0.5512 (0.5650)	total_loss 9.0162 (9.1025)	loss 4.2359 (4.2764)	multi_label_loss 4.7804 (4.8261)	grad_norm 28.2898 (inf)	mem 11359MB
[2023-11-07 14:10:00 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1500/45178]	eta 6:51:17 lr 0.000000	time 0.5718 (0.5650)	total_loss 8.9876 (9.1018)	loss 4.2348 (4.2760)	multi_label_loss 4.7528 (4.8258)	grad_norm 27.9185 (inf)	mem 11359MB
[2023-11-07 14:10:06 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1510/45178]	eta 6:51:20 lr 0.000000	time 0.5499 (0.5652)	total_loss 9.0538 (9.1009)	loss 4.2805 (4.2755)	multi_label_loss 4.7733 (4.8254)	grad_norm 29.0323 (inf)	mem 11359MB
[2023-11-07 14:10:12 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1520/45178]	eta 6:51:25 lr 0.000000	time 0.5652 (0.5654)	total_loss 9.0265 (9.1000)	loss 4.2458 (4.2750)	multi_label_loss 4.7807 (4.8250)	grad_norm 29.2532 (inf)	mem 11359MB
[2023-11-07 14:10:18 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1530/45178]	eta 6:51:21 lr 0.000000	time 0.6593 (0.5655)	total_loss 8.9603 (9.0991)	loss 4.2163 (4.2745)	multi_label_loss 4.7440 (4.8246)	grad_norm 29.1916 (inf)	mem 11359MB
[2023-11-07 14:10:24 group_vit_gcc_yfcc_30e_bs64x1] (main_group_vit.py 286): INFO Train: [0/30][1540/45178]	eta 6:51:18 lr 0.000000	time 0.5517 (0.5655)	total_loss 8.9539 (9.0982)	loss 4.1660 (4.2740)	multi_label_loss 4.7878 (4.8242)	grad_norm 27.5196 (inf)	mem 11359MB
