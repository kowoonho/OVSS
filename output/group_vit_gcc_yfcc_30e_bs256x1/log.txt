[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred_label
- final_group
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f9b223be310>
[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 15:54:27 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 15:54:28 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 15:55:42 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 15:55:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 15:55:42 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:00:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:00:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- all_groups
- input_pred_seg_label
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:00:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fc1b3d0ebd0>
[2023-10-31 16:00:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:00:57 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:00:57 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:00:57 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:00:57 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:00:57 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:02:09 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:02:09 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:02:10 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:03:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:03:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- all_groups
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:03:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7faaecfdbc50>
[2023-10-31 16:03:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:03:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:03:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:03:04 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:03:04 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:03:04 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- pred
- input_seg
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f6c7024f850>
[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:06:59 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:07:00 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:07:00 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:08:14 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:08:14 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:08:14 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:19:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:19:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:19:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f9ba05aee10>
[2023-10-31 16:19:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:19:52 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:19:52 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:19:52 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:19:52 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:19:52 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:21:04 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:21:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fe6f28995d0>
[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:22:51 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:22:52 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:22:52 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:24:07 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:24:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:24:07 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:26:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:26:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred_label
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:26:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f4d526b3450>
[2023-10-31 16:26:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:26:43 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:26:43 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:26:43 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:26:43 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:26:43 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:27:56 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:27:57 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:27:57 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:29:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:29:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred_label
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:29:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fb1a0f4a790>
[2023-10-31 16:29:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:29:06 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:29:06 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:29:06 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:29:06 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:29:06 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:30:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:30:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred_label
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:30:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f36fee8d150>
[2023-10-31 16:30:05 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:30:06 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:30:06 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:30:06 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:30:06 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:30:06 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:31:20 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:31:20 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:31:20 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:38:47 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:38:47 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 188): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred_label
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:38:47 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fa2ce6c4f50>
[2023-10-31 16:38:47 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:38:48 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:38:48 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:38:48 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:38:48 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:38:48 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:40:03 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:40:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:40:03 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:41:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 188): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred_label
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fc81ffce910>
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-10-31 16:41:35 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-10-31 16:42:54 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-10-31 16:42:54 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-10-31 16:42:54 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 13:17:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 13:17:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- input_pred
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 13:17:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fdc00833d10>
[2023-11-01 13:17:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-01 13:17:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-01 13:17:35 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-11-01 13:17:35 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-01 13:17:35 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-01 13:17:35 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 13:18:49 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-11-01 13:18:49 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-11-01 13:18:49 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis:
- pred
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fa7f2782d50>
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-01 13:20:26 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-01 13:20:27 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 13:21:44 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 394): INFO Eval Seg mIoU 52.29
[2023-11-01 13:21:44 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 96): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-11-01 13:21:44 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 21:57:08 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 21:57:08 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 21:57:36 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 21:57:36 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 21:58:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 21:58:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 21:59:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 21:59:27 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 21:59:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 21:59:42 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:09:09 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:09:09 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:09:40 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:09:40 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 79): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7feb50b2da10>
[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 81): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 84): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 90): INFO number of params: 55726610
[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-01 22:10:07 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-01 22:10:08 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 22:10:43 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 183): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:10:43 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 186): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:11:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:11:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 187): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:41:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:41:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:42:50 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 182): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:42:50 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 185): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:45:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:45:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:46:55 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:46:55 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:50:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:50:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:50:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f78b1997850>
[2023-11-01 22:50:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-01 22:50:30 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-01 22:50:30 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-01 22:50:30 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-01 22:50:30 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-01 22:50:30 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 22:52:01 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 396): INFO Eval Seg mIoU 52.29
[2023-11-01 22:52:01 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 95): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-11-01 22:56:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 22:56:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 22:56:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f69bf08eb10>
[2023-11-01 22:56:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-01 22:56:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-01 22:56:04 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-01 22:56:04 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-01 22:56:04 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-01 22:56:04 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fe7037ee890>
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-01 23:12:32 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-01 23:12:33 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 13:19:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 13:19:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 13:19:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fa620999b90>
[2023-11-02 13:19:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 13:19:08 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 13:19:08 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 13:19:08 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 13:19:08 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 13:19:08 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 15:58:13 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 15:58:13 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 15:58:13 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f6562628750>
[2023-11-02 15:58:13 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 15:58:14 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 15:58:14 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 15:58:14 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 15:58:14 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 15:58:14 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 16:02:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 16:02:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 16:02:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f34fce6f350>
[2023-11-02 16:02:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 16:02:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 16:02:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 16:02:24 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 16:02:24 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 16:02:24 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:17:49 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:17:49 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:17:49 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f2d50179550>
[2023-11-02 17:17:49 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:17:50 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:17:50 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:17:50 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:17:50 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:17:50 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f6b11d4a750>
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:22:03 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:22:04 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:25:22 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:25:22 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:25:22 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f17e027bbd0>
[2023-11-02 17:25:22 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:25:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:25:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:25:23 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:25:23 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:25:23 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f9c852eb690>
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:27:59 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:28:00 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f7d8092f790>
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:31:29 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:31:30 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:33:23 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f01034bf3d0>
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:33:24 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:36:15 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:36:15 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:36:15 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7ff7829d8410>
[2023-11-02 17:36:15 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:36:16 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:36:16 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:36:16 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:36:16 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:36:16 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:37:53 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:37:53 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:37:53 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f7f22319850>
[2023-11-02 17:37:53 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:37:54 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:37:54 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:37:54 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:37:54 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:37:54 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:39:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:39:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:39:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f9ff4085410>
[2023-11-02 17:39:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:39:52 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:39:52 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:39:52 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:39:52 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:39:52 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-02 17:42:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-02 17:42:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-02 17:42:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fd191d63490>
[2023-11-02 17:42:07 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-02 17:42:08 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-02 17:42:08 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-02 17:42:08 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-02 17:42:08 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-02 17:42:08 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 09:53:38 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 09:53:38 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 09:53:38 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f841f5d5390>
[2023-11-03 09:53:38 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 09:53:39 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 09:53:39 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 09:53:39 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 09:53:39 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 09:53:39 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f036284c450>
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:01:25 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:01:26 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:07:46 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:07:46 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:07:46 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f94e05dd310>
[2023-11-03 10:07:46 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:07:47 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:07:47 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:07:47 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:07:47 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:07:47 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:10:33 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:10:33 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:10:33 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f48fef5e0d0>
[2023-11-03 10:10:33 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:10:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:10:34 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:10:34 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:10:34 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:10:34 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:20:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:20:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:20:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f034524fd90>
[2023-11-03 10:20:25 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:20:26 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:20:26 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:20:26 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:20:26 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7f147d8543d0>
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:24:56 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:24:57 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:26:10 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 400): INFO Eval Seg mIoU 52.29
[2023-11-03 10:26:10 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 95): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fb7b035bf90>
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:34:29 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:34:30 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:35:43 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 400): INFO Eval Seg mIoU 52.29
[2023-11-03 10:35:43 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 95): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 181): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 184): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0016
  weight_decay: 0.05
  warmup_lr: 4.0e-06
  min_lr: 4.0e-05
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: true
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 78): INFO Evaluating dataset: <segmentation.datasets.pascal_voc.PascalVOCDataset object at 0x7fea8ff9ba90>
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 80): INFO Creating model:MultiLabelContrastive/group_vit_gcc_yfcc_30e_bs256x1
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 83): INFO MultiLabelContrastive(
  (img_encoder): GroupViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): GroupingLayer(
        dim=384, 
        input_resolution=196, 
        depth=6, 
        num_group_token=64, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=64, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
      )
      (1): GroupingLayer(
        dim=384, 
        input_resolution=64, 
        depth=3, 
        num_group_token=8, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): GroupingBlock(
          hard=True, 
          gumbel=True, 
          sum_assign=False, 
          num_output_group=8, 
           
          (norm_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_inter): Mlp(
            (fc1): Linear(in_features=8, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (norm_post_tokens): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (norm_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (pre_assign_attn): CrossAttnBlock(
            (norm_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (norm_q): Identity()
            (norm_k): Identity()
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=False
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (assign): AssignAttention(
            num_heads: 1, 
            hard: True, 
            gumbel: True, 
            sum_assign=False, 
            gumbel_tau: 1.0, 
            assign_eps: 1.0
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (k_proj): Linear(in_features=384, out_features=384, bias=True)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (norm_new_x): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp_channels): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
          (reduction): Identity()
        )
        (group_projector): Sequential(
          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (1): MixerMlp(
            (fc1): Linear(in_features=64, out_features=192, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=192, out_features=8, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (2): GroupingLayer(
        dim=384, 
        input_resolution=8, 
        depth=3, 
        num_group_token=0, 
        
        (blocks): ModuleList(
          (0): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): AttnBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              num_heads=6, 
              qkv_bias=0.125, 
              qkv_fuse=True
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (cross_entropy): CrossEntropyLoss()
  (soft_cross_entropy): SoftTargetCrossEntropy()
  (img_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(384, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
  (text_projector): ProjectMLP(
    (linear_hidden): Sequential(
      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,))
      (1): SyncBatchNorm(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (linear_out): Conv1d(4096, 256, kernel_size=(1,), stride=(1,))
  )
)
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 89): INFO number of params: 55726610
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 48): INFO ==============> Resuming form /workspace/Dataset/pre-trained_weights/groupvit/group_vit_gcc_yfcc_30e-879422e0.pth....................
[2023-11-03 10:38:37 group_vit_gcc_yfcc_30e_bs256x1] (checkpoint.py 51): INFO <All keys matched successfully>
[2023-11-03 10:38:38 group_vit_gcc_yfcc_30e_bs256x1] (group_vit_seg.py 139): INFO Building GroupViTSegInference with 21 classes, test_cfg=Config (path: None): {'bg_thresh': 0.95, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (448, 448)}, with_bg=True
[2023-11-03 10:39:51 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 400): INFO Eval Seg mIoU 52.29
[2023-11-03 10:39:51 group_vit_gcc_yfcc_30e_bs256x1] (main_seg.py 95): INFO mIoU of the network on the 1449 test images: 52.29%
[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 440): INFO Scale base_lr from 0.0016 to 0.0001
[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 441): INFO Scale warmup_lr from 4e-06 to 2.5e-07
[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 442): INFO Scale min_lr from 4e-05 to 2.5e-06
[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 450): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 456): INFO Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.8.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0+cu111
OpenCV: 4.4.0
MMCV: 1.3.14
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
------------------------------------------------------------

[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 458): INFO Git hash: unknown
[2023-11-06 11:55:11 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 461): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0001
  weight_decay: 0.05
  warmup_lr: 2.5e-07
  min_lr: 2.5e-06
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: false
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: ''
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 443): INFO Scale base_lr from 0.0016 to 0.0001
[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 444): INFO Scale warmup_lr from 4e-06 to 2.5e-07
[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 445): INFO Scale min_lr from 4e-05 to 2.5e-06
[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 453): INFO Full config saved to output/group_vit_gcc_yfcc_30e_bs256x1/config.json
[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 459): INFO Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.16 (default, Jan 17 2023, 22:20:44) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.8.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0+cu111
OpenCV: 4.4.0
MMCV: 1.3.14
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
------------------------------------------------------------

[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 461): INFO Git hash: unknown
[2023-11-06 11:57:06 group_vit_gcc_yfcc_30e_bs256x1] (main_group_vit.py 464): INFO data:
  batch_size: 256
  pin_memory: true
  num_workers: 6
  shuffle_buffer: 10000
  seed: ${train.seed}
  dataset:
    meta:
      gcc3m:
        type: img_txt_pair
        path: local_data/gcc3m_shards
        prefix: gcc-train-{000000..00436}.tar
        length: 2891445
      gcc12m:
        type: img_txt_pair
        path: local_data/gcc12m_shards
        prefix: gcc-conceptual-12m-{000000..001943}.tar
        length: 11156203
      yfcc14m:
        type: img_txt_pair
        path: local_data/yfcc14m_shards
        prefix: yfcc14m-{000000..001888}.tar
        length: 14615499
      redcap12m:
        type: img_txt_pair
        path: local_data/redcap12m_shards
        prefix: redcap12m-{000000..001211}.tar
        length: 11866987
      imagenet:
        type: img_cls_pair
        path: local_data/imagenet_shards
        prefix: imagenet-val-{000000..000049}.tar
        length: 50000
    train:
    - gcc3m
    - gcc12m
    - yfcc14m
    val:
    - imagenet
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale:
    - 0.08
    - 1.0
    interpolation: bilinear
    color_jitter: 0.4
    auto_augment: rand-m9-mstd0.5-inc1
    re_prob: 0.25
    re_mode: pixel
    re_count: 1
  text_aug:
    max_seq_len: 77
    multi_label: 3
    word_type: noun
train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 0.0001
  weight_decay: 0.05
  warmup_lr: 2.5e-07
  min_lr: 2.5e-06
  clip_grad: 5.0
  accumulation_steps: 0
  amp_opt_level: O1
  seed: 0
  lr_scheduler:
    name: cosine
  optimizer:
    name: adamw
    eps: 1.0e-08
    betas:
    - 0.9
    - 0.999
evaluate:
  eval_only: false
  eval_freq: 1
  task:
  - cls
  - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []
checkpoint:
  auto_resume: true
  resume: ''
  freq: 1
  max_kept: -1
  save_freq: 1
model_name: group_vit_gcc_yfcc_30e_bs256x1
output: output/group_vit_gcc_yfcc_30e_bs256x1
tag: default
print_freq: 10
seed: 0
wandb: false
local_rank: 0
vis: []
_base_: default.yml
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 384
    num_heads:
    - 6
    - 6
    - 6
    depths:
    - 6
    - 3
    - 3
    num_group_tokens:
    - 64
    - 8
    - 0
    num_output_groups:
    - 64
    - 8
    drop_rate: 0.0
    drop_path_rate: 0.1
  text_encoder:
    type: TextTransformer
    context_length: 77
    width: 256
    layers: 12
    vocab_size: 49408
  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

